{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import pyrootutils\n",
    "\n",
    "pyrootutils.setup_root(os.curdir, indicator=\".project-root\", pythonpath=True)\n",
    "from extras.paths import *\n",
    "from extras.constants import *\n",
    "from src.data import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_EN_PATH, mode='r') as f:\n",
    "    train_en = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Res@@ um@@ ption of the session',\n",
       " 'I declare resumed the session of the European Parliament ad@@ jour@@ ned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period .',\n",
       " 'Although , as you will have seen , the d@@ read@@ ed &apos; millenni@@ um bug &apos; failed to materi@@ alise , still the people in a number of countries suffered a series of natural disasters that truly were d@@ read@@ ful .',\n",
       " 'You have requested a debate on this subject in the course of the next few days , during this part-session .',\n",
       " 'In the meantime , I should like to observe a minute &apos; s silence , as a number of Members have requested , on behalf of all the victims concerned , particularly those of the terrible stor@@ ms , in the various countries of the European Union .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DE_PATH, mode='r') as f:\n",
    "    train_de = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiederaufnahme der Sitzungsperiode',\n",
       " 'Ich erklär@@ e die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene Sitzungsperiode des Europäischen Parlaments für wieder@@ aufgenommen , wünsche Ihnen nochmals alles Gute zum Jahres@@ wechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der ge@@ für@@ chtete &quot; Mill@@ en@@ i@@ um-@@ Bu@@ g &quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der St@@ ür@@ me , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schwei@@ ge@@ minute zu ge@@ denken .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_de[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500962"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = []\n",
    "for path in TEST_EN_PATHS:\n",
    "    with open(path) as f:\n",
    "        test_en += [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prague Stock Market falls to min@@ us by the end of the trading day',\n",
       " 'After a sharp drop in the morning , the Prague Stock Market corrected its losses .',\n",
       " 'Trans@@ actions with stocks from the Czech Energy Enterprise ( Č@@ E@@ Z ) reached nearly half of the regular daily trading .',\n",
       " 'The Prague Stock Market immediately continued its fall from Monday at the beginning of Tuesday &apos;s trading , when it dropped by nearly six percent .',\n",
       " 'This time the fall in stocks on Wall Street is responsible for the drop .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22140"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_de = []\n",
    "for path in TEST_DE_PATHS:\n",
    "    with open(path) as f:\n",
    "        test_de += [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die Pra@@ ger Börse st@@ ürzt gegen Geschäfts@@ schluss ins Min@@ us .',\n",
       " 'Nach dem stei@@ len Ab@@ fall am Morgen konnte die Pra@@ ger Börse die Verluste korrigieren .',\n",
       " 'Die Transaktionen mit den Aktien von Č@@ E@@ Z erreichten fast die Hälfte des normalen Tages@@ geschäf@@ ts .',\n",
       " 'Die Pra@@ ger Börse knü@@ pf@@ te gleich zu Beginn der Dienst@@ ag@@ s@@ geschäfte an den Ein@@ bruch vom Montag an , als sie um weitere sechs Prozent@@ punkte s@@ ank .',\n",
       " 'Dies@@ mal lag der Grund für den Ein@@ bruch an der Wall Street .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_de[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_EN_PATH, 'w') as f:\n",
    "    f.write('\\n'.join(test_en))\n",
    "with open(TEST_DE_PATH, 'w') as f:\n",
    "    f.write('\\n'.join(test_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_EN_PATH, mode='r') as f:\n",
    "    test_en_ = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(test_en, test_en_):\n",
    "    assert a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi30k translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Multi30k(split=('train', 'valid', 'test'), language_pair=('de', 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe,\n",
       " ShardingFilterIterDataPipe)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(datasets[0], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'Zwei Männer stehen am Herd und bereiten Essen zu.', 'Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'Ein Mann lächelt einen ausgestopften Löwen an.', 'Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'Jungen tanzen mitten in der Nacht auf Pfosten.', 'Eine Ballettklasse mit fünf Mädchen, die nacheinander springen.', 'Vier Typen, von denen drei Hüte tragen und einer nicht, springen oben in einem Treppenhaus.', 'Ein schwarzer Hund und ein gefleckter Hund kämpfen.', 'Ein Mann in einer neongrünen und orangefarbenen Uniform fährt auf einem grünen Traktor.', 'Mehrere Frauen warten in einer Stadt im Freien.', 'Eine Frau mit schwarzem Oberteil und Brille streut Puderzucker auf einem Gugelhupf.', 'Ein kleines Mädchen sitzt vor einem großen gemalten Regenbogen.', 'Ein Mann liegt auf der Bank, an die auch ein weißer Hund angebunden ist.', 'Fünf Personen sitzen mit Instrumenten im Kreis.', 'Eine Gruppe älterer Frauen spielt zusammen Klarinette von Notenblättern.', 'Ein großes Bauwerk ist kaputt gegangen und liegt auf einer Fahrbahn.', 'Eine große Menschenmenge steht außen vor dem Eingang einer Metrostation.', 'Ein Mann, der ein Tattoo auf seinem Rücken erhält.', 'Zwei Kinder sitzen auf einer kleinen Wippe im Sand.', 'Ein Mann, der eine reflektierende Weste und einen Schutzhelm trägt, hält eine Flagge in die Straße.', 'Eine Person in einem blauen Mantel steht auf einem belebten Gehweg und betrachtet ein Gemälde einer Straßenszene.', 'Ein Mann in grünen Hosen läuft die Straße entlang.', 'Das kleine Kind klettert an roten Seilen auf einem Spielplatz.', 'Du weißt, dass ich aussehe wie Justin Bieber.', 'Ein junger Mann in einer schwarz-gelben Jacke blickt etwas an und lächelt.', 'Ein Mann, der mit einer Tasse Kaffee an einem Urinal steht.', 'Fünf gehende Personen mit einem mehrfarbigen Himmel im Hintergrund.')\n",
      "('Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.', 'A man in a blue shirt is standing on a ladder cleaning a window.', 'Two men are at the stove preparing food.', 'A man in green holds a guitar while the other man observes his shirt.', 'A man is smiling at a stuffed lion', 'A trendy girl talking on her cellphone while gliding slowly down the street.', 'A woman with a large purse is walking by a gate.', 'Boys dancing on poles in the middle of the night.', 'A ballet class of five girls jumping in sequence.', 'Four guys three wearing hats one not are jumping at the top of a staircase.', 'A black dog and a spotted dog are fighting', 'A man in a neon green and orange uniform is driving on a green tractor.', 'Several women wait outside in a city.', 'A lady in a black top with glasses is sprinkling powdered sugar on a bundt cake.', 'A little girl is sitting in front of a large painted rainbow.', 'A man lays on the bench to which a white dog is also tied.', 'Five people are sitting in a circle with instruments.', 'A bunch of elderly women play their clarinets together as they read off sheet music.', 'A large structure has broken and is laying in a roadway.', 'A large crowd of people stand outside in front of the entrance to a Metro station.', 'A man getting a tattoo on his back.', 'Two children sit on a small seesaw in the sand.', 'A man wearing a reflective vest and a hard hat holds a flag in the road', 'A person dressed in a blue coat is standing in on a busy sidewalk, studying painting of a street scene.', 'A man in green pants walking down the road.', 'The small child climbs on a red ropes on a playground.', 'You know i am looking like Justin Bieber.', 'A young man in a black and yellow jacket is gazing at something and smiling.', 'A man standing at a urinal with a coffee cup.', 'Five people walking with a multicolored sky in the background.')\n"
     ]
    }
   ],
   "source": [
    "for de_text_batch, en_text_batch in loader:\n",
    "    print(de_text_batch)\n",
    "    print(en_text_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('Hello, world!')\n",
    "[token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = spacy.load('de_core_news_sm')\n",
    "en_tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "dataset = Multi30k(split='train', language_pair=('de', 'en'))\n",
    "\n",
    "for de_text, en_text in dataset:\n",
    "    de_tokens = de_tokenizer(de_text)\n",
    "    en_tokens = en_tokenizer(en_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### english counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "datasets = Multi30k(split='train', language_pair=('en', 'de'))\n",
    "src_texts, tgt_texts = list(zip(*datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "texts = src_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29001/29001 [00:16<00:00, 1717.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for tokens in tqdm(tokenizer.pipe(texts), total=len(texts)):\n",
    "    counter.update([token.text for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "vocab_path = Path('./vocab.txt')\n",
    "with vocab_path.open(mode='w') as f:\n",
    "    for token, count in counter.most_common():\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "vocab_path = Path('./vocab.txt')\n",
    "with vocab_path.open(mode='r') as f:\n",
    "    vocab = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "tokens = SPECIAL_TOKENS + vocab\n",
    "lookup = {token: i for i, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "class Vocab:\n",
    "    SPECIAL_TOKENS = {\n",
    "        'UNK': '<unk>',\n",
    "        'PAD': '<pad>',\n",
    "        'SOS': '<sos>',\n",
    "        'EOS': '<eos>'\n",
    "    }\n",
    "    SPECIAL_TOKENS_ORDER = ['UNK', 'PAD', 'SOS', 'EOS']\n",
    "    SPECIAL_TOKENS_IDX = {token: i for i, token in enumerate(SPECIAL_TOKENS_ORDER)}\n",
    "\n",
    "    def __init__(self, vocab_path, language):\n",
    "        self.vocab = Vocab.load_vocab(vocab_path, language)\n",
    "        self.lookup = {token: i for i, token in enumerate(self.vocab)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        return [self.lookup.get(token, Vocab.SPECIAL_TOKENS_IDX['UNK']) for token in tokens]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path, language):\n",
    "        if vocab_path.exists():\n",
    "            with vocab_path.open(mode='r') as f:\n",
    "                vocab = f.read().splitlines()\n",
    "        else:\n",
    "            vocab = Vocab.build_vocab(vocab_path, language)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(vocab_path, language):\n",
    "        datasets = Multi30k(split='train', language_pair=('en', 'de'))\n",
    "        src_texts, tgt_texts = list(zip(*datasets))\n",
    "\n",
    "        if language == 'en':\n",
    "            tokenizer = spacy.load('en_core_web_sm')\n",
    "            texts = src_texts\n",
    "        else:\n",
    "            tokenizer = spacy.load('de_core_news_sm')\n",
    "            texts = tgt_texts\n",
    "            \n",
    "        counter = Counter()\n",
    "        for tokens in tqdm(tokenizer.pipe(texts), total=len(texts)):\n",
    "            counter.update([token.text for token in tokens])\n",
    "        \n",
    "        vocab = [Vocab.SPECIAL_TOKENS[special_token] for special_token in Vocab.SPECIAL_TOKENS_ORDER]\n",
    "        with vocab_path.open(mode='w') as f:\n",
    "            for token, count in counter.most_common():\n",
    "                f.write(token + '\\n')\n",
    "                vocab += [token]\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "en_vocab = Vocab(vocab_path=Path('./en_vocab.txt'), language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 11, 1857, 1224]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab(['Hel', ',', 'world', '!'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
